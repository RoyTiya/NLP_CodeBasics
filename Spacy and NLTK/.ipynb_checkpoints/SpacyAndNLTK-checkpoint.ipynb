{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b9a6bbe-5e8f-4f72-9ac0-c3627d9fe3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b0ee86-454b-4893-b627-7abc7b66776c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr stranger loves samosa.\n",
      "Hulk loves chaat of Delhi\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Dr stranger loves samosa. Hulk loves chaat of Delhi\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f04e2b-e10e-480b-b8f8-747b7a297b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr\n",
      "stranger\n",
      "loves\n",
      "samosa\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "chaat\n",
      "of\n",
      "Delhi\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sents:\n",
    "    for word in sentence:\n",
    "        print(word)\n",
    "    # print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9677e6-c2cc-47cb-b79d-4f460d9e4790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\envs\\nlp\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5f34de-099c-480f-85ea-3f64b643b5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fa887d0-d3f4-4ab4-b13b-69613aa331da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. stranger loves samosa.', 'Mr. Hulk loves chaat of Delhi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(\"Dr. stranger loves samosa. Mr. Hulk loves chaat of Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cac9137-d0ea-4742-9f01-b076440a046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'stranger',\n",
       " 'loves',\n",
       " 'samosa',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Hulk',\n",
       " 'loves',\n",
       " 'chaat',\n",
       " 'of',\n",
       " 'Delhi']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(\"Dr. stranger loves samosa. Mr. Hulk loves chaat of Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1104928c-a648-463d-802c-3834017b5bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "two\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "s = \"Dr. Strange loves pav bhaji of mumbai as it costs two $ per plate.\"\n",
    "    # for token in s:\n",
    "    #     print(token)\n",
    "        \n",
    "doc = nlp(s)\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7ce24bf-2f5b-4f5a-bf62-b93b47cbf965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strange"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[1]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4699710a-99d8-4a55-a2a3-9d8a61f3bc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e6591f4-ef75-4e70-8066-7b25c86d205f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_kb_id',\n",
       " 'ent_kb_id_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_dep',\n",
       " 'has_extension',\n",
       " 'has_head',\n",
       " 'has_morph',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'iob_strings',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_end',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'morph',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'remove_extension',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'set_morph',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd392893-a38f-42d5-9d48-3e4ff1589f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e0be598-6c38-4e85-8eb9-a083812db7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1 = doc[-5]\n",
    "token1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d55f59ba-48ef-4fae-91eb-1bb192ab210e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token1.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62cb4ae9-c905-4a7a-a9c2-ca0f2892c9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[-4]\n",
    "token2\n",
    "token2.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34cc5568-ef0f-4572-89bb-4ccf7de3d62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. ==>  index: 0  || is_alpha: False  || is_punct: False  || like_num: False  || is_currency: False\n",
      "Strange ==>  index: 1  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "loves ==>  index: 2  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "pav ==>  index: 3  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "bhaji ==>  index: 4  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "of ==>  index: 5  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "mumbai ==>  index: 6  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "as ==>  index: 7  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "it ==>  index: 8  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "costs ==>  index: 9  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "two ==>  index: 10  || is_alpha: True  || is_punct: False  || like_num: True  || is_currency: False\n",
      "$ ==>  index: 11  || is_alpha: False  || is_punct: False  || like_num: False  || is_currency: True\n",
      "per ==>  index: 12  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      "plate ==>  index: 13  || is_alpha: True  || is_punct: False  || like_num: False  || is_currency: False\n",
      ". ==>  index: 14  || is_alpha: False  || is_punct: True  || like_num: False  || is_currency: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==> \",\"index:\", token.i,\n",
    "          \" || is_alpha:\", token.is_alpha, \n",
    "          \" || is_punct:\", token.is_punct, \n",
    "          \" || like_num:\", token.like_num, \n",
    "          \" || is_currency:\", token.is_currency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965af01e-253c-4194-a608-64ffb242d931",
   "metadata": {},
   "source": [
    "### Collecting email IDs from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92d18885-75c5-4012-9833-2e813b946c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\\\n\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name\\tbirth day\\temail\\n',\n",
       " '\\t\\t\\t\\t\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com\\n',\n",
       " 'Joe     1 May, 1997    joe@root.com\\n']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Student.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "742fd1e9-3814-4f3c-a2a9-a8c1dbf0996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\\\n\\n ==================================================\\n \\n Name\\tbirth day\\temail\\n \\t\\t\\t\\t\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com\\n Joe     1 May, 1997    joe@root.com\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3bf9f37-04c9-4930-b6ca-3402a2e7f826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c3d065-4d68-4fea-ac93-2f8da1b9fbd6",
   "metadata": {},
   "source": [
    "### Using Spacy in Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8ac12ef-e51e-486e-89bd-c4f9278a0413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कैसी False False\n",
      "हो False False\n",
      "वृंदा False False\n",
      "? False False\n",
      "जो False False\n",
      "पैसा False False\n",
      "₹ True False\n",
      "5000 False True\n",
      "मैंने False False\n",
      "उधार False False\n",
      "लिया False False\n",
      "था False False\n",
      "वह False False\n",
      "खो False False\n",
      "गया False False\n",
      "है False False\n",
      "। False False\n"
     ]
    }
   ],
   "source": [
    "strng = \"कैसी हो वृंदा? जो पैसा ₹5000 मैंने उधार लिया था वह खो गया है।\"\n",
    "nlp = spacy.blank(\"hi\")\n",
    "doc = nlp(strng)\n",
    "for token in doc:\n",
    "    print(token, token.is_currency, token.like_num )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb6383-3ac5-4250-9ff2-3ba37dcb99e4",
   "metadata": {},
   "source": [
    "### Customizing tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4760d0c1-9c38-424e-9dea-fd7c9017134b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6910f85-def2-4a97-ba74-0f7a9019ace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [{ORTH: \"gim\"}, {ORTH: \"me\"}])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba44b9c-3ca2-4189-8d0e-06e93a087069",
   "metadata": {},
   "source": [
    "### Exercise\r\n",
    "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\r\n",
    "\r\n",
    "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "45655a7f-47df-41fd-a13a-b4e32ecd813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: token has an attribute that can be used to detect a url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eff6d453-06f6-42a0-bf5e-8243f6afc2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Look for data to help you address the question. Governments are good\n",
       "sources because data from public research is often freely available. Good\n",
       "places to start include http://www.data.gov/, and http://www.science.\n",
       "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
       "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
       "and the European Social Survey at http://www.europeansocialsurvey.org/."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = \" \".join(text)\n",
    "doc = nlp(text)\n",
    "doc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eac5d556-adb7-49e5-86a1-98db372092ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = []\n",
    "for token in doc:\n",
    "    if token.like_url:\n",
    "        url.append(token.text)\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f1562-a96c-49a7-8863-0a7a7ea31b74",
   "metadata": {},
   "source": [
    "(2) Extract all money transaction from below sentence along with currency. Output should be,\r\n",
    "\r\n",
    "two $\r\n",
    "\r\n",
    "500 €"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8447eb3c-6790-4f72-976f-980ef0bd2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "\n",
    "# TODO: Write code here\n",
    "# Hint: Use token.i for the index of a token and token.is_currency for currency symbol detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1024c472-3d1a-4d44-837f-7526ce32c75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony gave two $ to Peter, Bruce gave 500 € to Steve"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(transactions)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1391463-a3dc-42fb-ac49-d5ce5d73d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony False\n",
      "gave False\n",
      "two False\n",
      "$ False\n",
      "to False\n",
      "Peter False\n",
      ", False\n",
      "Bruce False\n",
      "gave False\n",
      "500 True\n",
      "€ False\n",
      "to False\n",
      "Steve False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = []\n",
    "for token in doc:\n",
    "    print(token, token.like_num)\n",
    "    # if token.is_digit:\n",
    "    #     url.append(token.text)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852f3a7-fb0c-458f-8876-26e5d456c96a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
